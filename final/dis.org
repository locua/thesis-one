* Title and document setup                                           :ignore:
#+options: h:3 num:t toc:nil \n:nil
#+latex_class: book
#+latex_header_extra: \input{config.tex}
#+latex_header: \input{mytitle}
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{emptypage}
#+LATEX_HEADER: \usepackage{enumitem}
# #+LATEX_HEADER: \usepackage[draft]{graphicx}

* other title ideas :noexport:
# #+title:w Describing systems for the exploration of tangible and spatial computer interaction  
# #+title: Spatial memory, embodied thinking, computer vision projection application \\
# #+title: or \\
# #+title: Exploring cognition and interaction in a spatial and physicalised computer environment. \\
# #+title: or \\
* Acknowledgements :ignore:
\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}

In no particular order thanks to Florent, Liz, Andrew, as well all my fellow
living companions in Ladywell who've come together in this unusual time. Special
thanks Jamie Forth for humoring the exprimental direction of this project and
offering diverse reference and support.

\end{abstract}
\newpage

* Abstract :ignore:
\renewcommand{\abstractname}{Abstract}
#+LaTeX: \begin{abstract}
Presented here is a specification for experimental approaches to computer
interaction based on spatial and tangible methods. The report describes a
prototypical implementation of a base system for computer interaction using
physical objects on a surface. A theoretical API (Application Programming
Interface) for such a system and similar systems is proposed. This utilises
computer vision techniques to analyse a surface to track objects and the
subsequent projection of graphics back onto the space creating a feedback system
for interaction. An attempt to gather together literature and examples of such
existing systems is made. The fundamental principle of the interaction model
described is summarised in the sentence below:

#+begin_quote
Physical objects on an /action surface/ have interactive properties; each object
is both a sensor and actuator.
#+end_quote


# ???An ethnomethodological framework for evaluation and further development
# is proposed???


#+LaTeX: \end{abstract}
* TOC and figures                                                    :ignore:
\tableofcontents
#+latex: \listoflistings \listoffigures
* Introduction

A project for the discussion and development of a comptuer vision based
interaction systems. Originally I (maybe ambitiously) aimed at creating a fully
, to explore the psychological aspects of interacting with computers
by moving objects around in a space. Motivated by years spent in some space or
another, hunched in front of a screen in all manner of contortions it is the aim
of this work to challenge this composition and contribute to something new. It
is an attempt to further expand interaction and assimilate computing into space.
\\

Presented here is a report on a open-source software prototype, a spatial
interaction system where coloured objects (in this case felt shapes), are
detected and located by a camera and projected back onto. This model of spatial
objects on an interaction surface is the proposed departure from the
keyboard-monitor paradigm. The floor (or table) becomes the monitor and objects
in the space the mouse. The potential for the use of a keyboard for text input
is also envisaged, and beyond that, any kind of controller that could be used in
this environment. \\

In no way an original idea, this project presents an open-source rendition of
this model using OpenFrameworks (a C++ toolkit for “creative coding”)(OF). This
library allowed for speedy assembly with out-of-the-box consolidation of various
graphics, computer-vision and GUI libraries. In this case ofxCv, ofxXmlSettings
and ofxGui alongside the built-in graphics functionality of OF. Due to the large
number of additional libraries and active community it seemed a positive
environment for future expansion and collaboration. \\

Alongside the software, an abstract formalisation is briefly described. This
considers this interaction model in a conceptual manner, so that it might be
approached from a multi-discplinary scientific perspective. Human-Computer
Interaction, as a discipline, lives at the cross roads of diverse fields such as
computing, design and behavioural science. Although the scope and context of
this project has not allowed for extra-disciplinary collaboration through the
presentation of this secondary component it aims to keep itself open to this
future possibility. \\

Originally further prototyping and development was planned in addition to
evaluation. This was proposed for evaluating end user interaction, such as
thorough user-testing. In its current form the potential userbase would be the
author and other software developers, interested in pursuing and developing this
research in an open-source way. The further reaching end goal is one of
prototyping software for diverse and non-technical users, essentially for any
computer user. A critical reference that I would like to put special note to
here is the work of Bret Victor and Alan Kay, particulary in the rather elusive
project Dynamicland (based in the US) and Victor's other writing and design.
This was a cornerstone of the contextual research, a vital point of reference
and inspiration for the project. \\

A contextual and historical N.B. \\

The original aims of the project were partly affected by the restrictions
brought about on daily life due to the Novel coronavirus pandemic.

** Other stuff :noexport:
An ethno-methodoligical framework is briefly discussed
users themselves.
* Background label:background

The motivation for this project stems in part from a feeling of frustration in
 how working on computers can often be a constricted and static affair. From
 this came a pondering of how one might expand, and move away from, the
 /keyboard-mouse-monitor/ model to improve the utility of computers regarding
 our physical health, wellbeing and perceptive abilities. How might a spatial,
 haptic and tangible environment for interaction create an improved space for
 working and thinking with computers as well with our physical health
 cite:TwengeJeanM2020Wiia? How might such an environment fundamentally augment
 our cognitive capabilities; memory and learning as well as creativity itself?
 
** Acronyms / definitions.
- CV - Computer vision
- HCI - Human Computer Interaction
- KMM - keyboard-mouse-monitor
- Exocortex - A software extension of a cognitive task.
- GUI - Graphical User Interface
** /Exocortices/ and augmented cognition

I started off looking at /Exocortices/ and other personal archiving systems.
Systems that allow the user to externalise thought and memory. This could be via
simply storing and organising work and ideas efficiently and methodically or
unifying many tasks or different workflows into a singular interface. Org mode
is a good example of such a system. Org mode is a "computing environment for
authoring mixed natural and computer language documents"
cite:Schulte:Davison:Dye:Dominik:2011:JSSOBK:v46i03. Designed for taking notes,
producing documents and organising, it runs inside of the text editor, Emacs. It
has the ability to export to different formats such as HTML, LaTeX and supports
"outlining, note-taking, hyperlinks, spreadsheets, to-do lists, project
planning, GTD" as well as literate programming, all in plain-text
cite:Schulte:Davison:Dye:Dominik:2011:JSSOBK:v46i03 [fn:2]. \\

Another point of reference when I was looking at externalised 'artificial
information-processing systems' was Devine Lu Linvega's Exocortex [[https://wiki.xxiivv.com/site/nataniev.html][XXIIV --
nataniev]]. /XXIIV/ is a personal archive and log with documentation of Linvega's
personal tools and artworks. Originally a static, javascript and lisp based
website with diaries, blog type posts and categorised personal logs, it is now
somewhat stripped back in style and has been rewritten in [[https://en.wikipedia.org/wiki/C99][C (C99)]]
cite:DevineNataniev. \\

Both these two systems have their own specific use-cases; /Org-mode/-- in
academia and /XXIIV/-- an experimental personal archive. They both utilise the
contemporary and prevailing /keyboard-mouse-monitor/ paradigm of computer
interaction to push the boundaries of cognition in this medium, particularly
regarding memory and productivity. These two projects were a birth point in
thinking about how software systems can augment thought and improve learning
ability and productivity. \\

# ** Nielsen: augmenting ltm and using ai to augment human-i ??????

Information visualisation is another tool for the amplifying cognition that most
take for granted. The externalisation and translation of data into shape and
colour allows us to see patterns not easily seen in listed data. Furthermore
utilising visualisation for memory tasks by organising attention and concept
mapping are useful ways to increase our abilities cite:WareColin2013Ivpf.

Scientist Michael Nielsen also offers some approaches to increasing long term
memory through the use of simple flash card software that orders things as you
review them by how well you know them. He suggests this and the process itself
of creating question/answer flashcards improves memory capacity, understanding
and our ability to do deep readings of a subject
cite:NielsenMich2018altm,carter2017using.

** A virtual exploration of a 'dynamic land'

Another principal point of reference was /Dynamicland/, a research project in
Oakland, USA. The aim of the project is to implement and research a new more
powerful and accessible model of computing.

#+begin_quote

In Oakland, we built the first full-scale realization of the vision, inviting
thousands of people into our space to collaborate. Together, these artists,
scientists, teachers, students, programmers, and non-programmers created
hundreds of projects that would have been impossible anywhere else.
-- Dynamicland.org 

#+end_quote


/Dynamicland/ is a communal computer where the building is the computer (ENIAC).
Programs are embodied in the room on pieces of colour-coded paper. The programs
are recognised via the codes and their code, stored in a database is then run,
it can also /read/ code using OCR but generally the code is there [[https://thenewstack.io/dynamicland-rethinks-computer-interfaces/][symbolically]].
Projectors on the ceiling transform the paper and workbenches into whatever the
programmer decides. This relatively simple model makes for an exciting new
ecosystem for collaborative computing and expressive programming. Victor
highlights his ideas for the progression of computing and interaction in a
series of talks (available online) and on his [[http://worrydream.com][website]]. In his talk "Seeing
Spaces" he describes a new kind of maker-space which allow makers to see across
time and possibilities. /Dynamicland/ seeks to offer a computational medium
which allows for full use of the human senses; a more [[https://vimeo.com/115154289][humane representation of
thought]] cite:VictorKayDynamicLand. \\

#+caption: RealtalkOS, the operating system of /Dynamicland/
#+ATTR_LATEX: :width 12cm
[[file:assets/realtalk-os.jpg]]  


/DL/ was a major inspiration for the main technical model for this project, an
/augmented/ workspace either on the floor or a table which is projected onto. A
camera/s pointing down onto the projection space is the sensor for detecting
interaction, with the projector as the actuator. This base model can be seen in
Figures ref:pp-schema and ref:systemSchema.

*** Dynamiclands opensource model :noexport:

** Paper programs 

Looking to find some of the code for /Dynamicland/ (DL) and a more detailed
specification of *DL* I stumbled across /Paper Programs/ (PP) ( /Dynamicland/
has an 'open-source model', but it is only open if you can visit it physically
as the source code is physically in the space). /Paper Programs/ (PP) is a
browser-based partial clone of /Dynamicland/. PP takes one element of
dynamicland, i.e. the representation of computer programs in a spatial
environment, on pieces of paper. Programs are written in Javascript and stored
in a Postgresql database. This idea of 'physicalizing' some method or element of
the computer and allowing the direct haptic manipulation of it has further
inspired this project. \\

#+ATTR_LATEX: :width 12cm  :float
#+caption: /Paperprograms/ in action label:pp-users
[[file:assets/pp_action2.png]]

PP aims, like Dynamicland, to create a collaborative programming environment
where anyone in the space can write Javascript programs and interact with
others. As in DL each program has a unique code and a colour encoding. It
follows the same basic hardware model. That being a projector and camera on the
ceiling and the paper "programs" (See Fig. ref:pp-schema.). This new vision of
collaborative computing and somewhat "multi-modal" interaction is one of the
initial inspirations and an important reference for this project.


#+caption: The initial physical schema: /Paperprograms/ label:pp-schema
#+ATTR_LATEX: :width 10cm :float
[[file:assets/pp-diag.png]]

** Tangible bits - Ishii and Ullmer

Another significant reference exploring novel approaches to interaction
involving physical objects was the paper: /Tangible bits: towards seamless
interfaces between people, bits and atoms/ (1997). It describes the motivation
for users to be able to "grasp and manipulate" bits, making them "tangible". The
paper also presents three prototypes, – the /metaDESK/, /transBOARD/ and
/ambientROOM/ and establish a new HCI approach "Tangible user interface[s]"
(TUI) with equivalence to Graphical user interfaces (GUI's) cite:IshiiH2002Tbdt.
It is an academic precursor to Dynamicland and is a starting point for tangible
interaction, merging ubiquitous-computing, augmented reality and
psychological approaches to HCI.

** Implementation and abstraction label:implement_and_abstraction

In the SAGE Handbook of Digital Technology Research's chapter on Haptic
interfaces design parameters are listed:

#+ATTR_LATEX: :options [noitemsep]
- Cutaneous Perception
- Frequency
- Duration
- Rhythm
- Location
- Intensity
- Texture
- Kinesthetic Perception
- ...

These parameters present considerations for the design of such interfaces but
also a formalisation of haptic interaction in the abstract
cite:HigginsSteve2015TSho. It takes the possible elements of 'hapticity' and
lays them out. This motivated a second outcome to the implementation itself, to
construct a /formal/ specification for spatial and tangible interaction so as to
describe the elements conceptually. This could then be used for further
development of similar systems and allow for multi-disciplinary scientific
experimentation. The benefits of having such a blueprint would be to present
spatiality and tangibility (in relation to HCI) formally so as to allow for
identification of elements for use. \\

Future researchability potential.
cite:LazarJonathan2017RMiH

*** notes :noexport:
Moving from implementation to abstraction

Ethnomethodology

Embodied Cognition

Haptic interfaces


- Touch is bi-directional, percieve and actuate via touch
  - Touch is an input and output tool in HCI
- Also can be active and passive. Exploration of object vs /passive/ eg
  vibrotactile actuators in a mobile phone vibrating when phone rings.
- Standardised keyboard shortcuts
- In cog sci looking to explore the phenomena on a cognitive level while in HCI
  approach we are looking to formalise the computational interaction system /
  schema
  
** Multi-modal interaction label:multimodalpro

#+caption: Multi-modal painting
#+ATTR_LATEX: :width 14cm 
file:assets/multi-modal-proj.jpg

An experimental [[https://locua.github.io/posts/install-y1.html][project]] I produced in 2017 has also informed the direction of
this project. This work was a multi-modal paint program; where hand movements
and facial expressions controlled different parameters of a paint program. This
included colour, size and position of the stroke. Additionally the different
modes of input were also controlling parameters on a looping synthesizer. The
installation was multi-modal in input and output. It was an artwork in outlook
and formed an initial experiment in designing interaction. The work was
particularly successful with children, who seemed to quickly get the hang of the
controls. It also included the combination of a variety of inputs to interaction
with a variety of outputs. Thought not necessarily the most effective or widely
applicable it explored the capabilities of some more unusual interactive modes.

#+caption: Multi modal schematic
#+ATTR_LATEX: :width 15cm
[[file:assets/multi-modal-proj-diag.jpg]
** MIT Prof - tangible media group                                :noexport:
http://tangible.media.mit.edu/projects/

** Computational creativity? :noexport:

* Specification and context
** Brief
To sum up the fundamental principle of the style of interaction that this
document aims to describe is summarised in the sentence below.

#+begin_quote
Physical objects on an /action surface/ have interactive properties; each object
is both a sensor and actuator.
#+end_quote

I provide this foundation so as to differentiate it from commonly used
contemporary systems. It highlights that a 'live' surface will act as a space
where objects are augmented with additional properties i.e. input and output to
a computer system. \\

** Technical 
As in the original specification the aim was to create a system for spatial
interaction. Initially I imagined it to work on a table top surface (in the end
it was developed on a floor mat due to considerations in my development
environment; see Chapter ref:projectindepth). The other principle component was
that interaction would be based on the placement and movement of objects around
the work-surface. The position and movements of these objects would be picked up
by a camera and actuated by a projector; both situated above the surface looking
down onto it. A horizontal setup would also be possible, with for example,
magnetised components keeping the objects to a board. Alongside the spatial
objects a computer keyboard may be used for additional input such as inputting
text or formatting. \\

The original specification involved using /Paper Programs/ and build on top of
this. With the /PP/ system, I planned to write a program/s to explore the
psychology of interaction with such a system. This could take the form of a
game-like psychology experiment. Rather than risk attempting a psychology
thesis, within a computing project focus has been put on creating and exploring
the implementation and formalisation of the interaction model itself. Due to
technical issues with /PP/ and the motivation to explore an alternative
interaction model, I decided to implement the system using [[https://openframeworks.cc/download/][*openFrameworks*]], a
C++ toolkit for experimental application development. I chose this framework as
it has straightforward 'out of the box' graphics capabilities as well as
numerous add-ons. These include /OpenCV/ cite:opencv_library wrappers and GUI
libraries as well as an active community of users. This combination in one
framework seemed suitable for quick experimentation and prototyping for this
project. Other C++ libraries were to be considered; Cinder and OpenCv as well as
just OpenCv. The physical setup would include a Projector and HD webcam and
computer for running the application. See Fig. ref:systemSchema for the software
and hardware schematic for this technical conception. \\

** Design considerations

An important design consideration that has driven this project is accessibility.
From my research into similar projects an aim was to create a platform, that
would be open source and easily setup, so that others could easily run and
further develop it. This was another reason for using [[https://openframeworks.cc/download/][openFrameworks]] which is
cross platform (Windows, OSx, IOS and Linux). This would mean with minor or no
modification of the code, it could be run on all the major desktop platforms.
The hardware requirements are also the kind which are either cheaply
(relatively) sourced or commonly available in educational institutions (one of
the target areas for which further development was envisioned). \\

Due to the limited scope of this project in both time and academic context a
secondary theoretical component is conceived[fn:1]. This is in the form of a
theoretical specification and API for this project and similar systems. As
discussed previously (ref:implement_and_abstraction) a set of parameters and
variables can form a useful part of a conceptual illustration and formalisation.
This would include diagrammatic illustrations of different classes representing
elements of the system, such as I/O and transformable objects. \\ 

#+caption: Abstract system schema label:abstractSystemSchema  
#+ATTR_LATEX: :width 11cm :float 
[[file:assets/abstract-system-schema.png]] 

The formalisation will address how various aspects of this interaction model can
distribute and externalise cognitive work. /Annotating/ (such as crossing out or
underlining) and /Cogntive tracing/ (manipulating items into different orders or
structures) are two methods for externalising cognition. These two methods and
others methods will be connected to elements of the interaction model.
cite:SharpHelen2019IDBH

** Users

As an academic and open-source software design project the intended audience for
the work can be split into two categories. This would be open-source developers
and technologists and academics working in the fields of HCI and other related
disciplines such as Cognitive Science and Psychology. \\

As an open-source project this project aims to attract programmers interested in
exploring new models for interaction. How can a desk or room be transformed into
a new interactive medium. Those with specialisations in different areas of
computing and beyond could contribute to different branches of advancement. To
present outcome as an open project gives scope for further development which the
scope and context of this thesis has not allowed for.

With the theoretical outcome an academic audience is intended. Scientific
exploration of the ideas in this report could allow for optimisation of the
purported benefits and modelling of interaction. Cross over between these two
above distinctions is also likely and this project hopes to sit at the
intersection of the two.

* Project in depth label:projectindepth

** Finalised design

After the testing of different software and approaches (detailed in Chapter
ref:creativeproc) the setup for the software outcome was chosen. This is
illustrated in Fig. ref:systemSchema. The hardware used was an *Epson EH-TW650*
*3LCD*, a *Logitech C920* HD Webcam and a laptop running Ubuntu Linux (18.04
LATS). The projector was secured to the ceiling with a mount and all cables were
extended to the floor. The projector setup can be seen in Appendix I, Fig
ref:projncam. All the source code can be found by following links in Appendix
II (ref:codelinks). \\

The software architecture consists of three classes:
#+ATTR_LATEX: :options [noitemsep]
- ~ofApp~, creates the GUI interface window with controls for tweaking CV
  settings and input parameters
- ~Projector~, this class creates the projector window.
- ~State~, this class stores variables that can be shared between the
  ~Projector~ and ~ofApp~ classes.

#+caption: Finalised system schema label:systemSchema  
#+ATTR_LATEX: :width 10cm
[[file:assets/project-schema-final.png]]

** Implementation details

*** Computer vision and fundamentals

The first essential component to get working was the computer vision. The core
of this involves blob tracking for each colour in the ~targetColours~ and
calling ~findContours~, passing in (by reference) the cropped pixel array using
the corresponding ~contourFinder~ object. Therefore, we loop five array, an
~ofPixels~ object containing the camera pixel data for the active detection
region.

#+caption: Computer Vision with ofxCv  label:ofxCvCv
#+begin_src cpp
// Check new frame 
if(cam.isFrameNew()) {
    // Loop for number of colours and track target colours
    for(int i = 0; i < num_colours; i++){
        // if finding: find // cv on / off
        if(ss->find) ss->contourFinders[i].findContours(camPix);
    }
}
#+end_src
\\ 

Five different colours were chosen as it is the same as in /PP/. Given its
identical hardware setup it seemed a good number. Having more colours means
thresholds will be lower so as to distinguish between less distinct colours; for
example pink and red. The contour finder has a number of parameters which allow
for fine grained control over the tracking. They are listed below:

#+ATTR_LATEX: :options [noitemsep]
- ~TargetColor~
- ~Threshold~
- ~MinArea~
- ~MaxArea~
- ~MinAreaRadius~
- ~MaxAreaRadius~
  
Architecturally the application is comprised of two windows the *GUI* and
*Projector*, represented in two classes ~ofApp~ and ~Projector~ respectively.
The *GUI* window is a control panel for the computer vision (CV) tracking.
Controls for the CV parameters are available in the *GUI* window, as handles to
crop the active region of the camera frame were the CV happens. In the
screenshot (Fig. ref:guiwindow) the tracking parameters are seen on the left and
the target colours are on the right. In the centre the rectangle with the pink
circles on upper left and bottom right corners is the active detection region.
\\

#+caption: GUI window. label:guiwindow 
#+ATTR_LATEX: :width 15cm
[[file:assets/gui-window2.jpg]] 

The other main window used is in the *Projector* class. This deals with the
display of the reaction surface. The crux of what this class achieves is in the
mapping and locating of the various colour blobs detected by the
~ContourFinder~'s. This is shown in the code block ref:pf. The ~contourFinders~
are accessed via the ~State~ class [fn:3]. All of the areas of interest are
looped over and their /centroids/ accessed. The locations are mapped to the
projector window size and aspect ratio and the colour index is stored. \\

#+caption: Crucial projector code. label:pf
#+begin_src cpp
for (auto j = 0; j < ss->contourFinders[i].getBoundingRects().size(); j++) {
  cv::Point2f p_;
  cv::Point3f p__;
  // Get centre of blob
  p_ = ss->contourFinders[i].getCenter(j);
  // map cropped camera to window
  p__.x = ofMap(p_.x, 0, ss->width_height.x, 0, mw);
  p__.y = ofMap(p_.y, 0, ss->width_height.y, 0, mh);
  // Store location and colour index
  p__.z=i;
  blobs.push_back(p__);
#+end_src

An example of detection and a corresponding projection can be seen in Appendix I
(Figure ref:serve_project).

*** Settings
To allow for tweaking and debugging during further development there is the
ability to save the settings of the computer vision parameters. This uses the
ofxXmlsettings addon. In the ~setup()~ method of the ~ofApp~ class we load and
loop over the settings. There is also a function, ~saveSettings()~, which allows
one to save settings at any time. This is assigned to the =s= key.

*** GUI keyboard shortcuts
The GUI interface some other functionality that it is relevant to briefly
describe. The keyboard shortcuts allow for various controls of the interface. A
chequerboard and corner markers can be toggled on the projector window. A simple
zoom mode can be enabled but is not very functional. There is also some
interfacing for v4l2-ctl, a CLI application for controlling the settings on the
camera. This allows for quick and dynamic controlling exposure and other
settings, which can be useful when getting an optimal image for colour and blob
detection. The full list of shortcuts is listed below.

#+ATTR_LATEX: :options [noitemsep]
- Toggle keyboard shortcuts with =k=
- Toggle tracking with =t=
- Toggle corners on projection window with =c= 
- Toggle chequerboard on projection window with =C=
  - Useful for keystone calibration
- Reset camera settings to default with =r=
- Toggle zoom mode with =z=
  - Doesn't work in a useful way
- Increment and decrement exposure with =+= and =-=
  - Only works if v4l2-ctl is installed 
- Toggle fullscreen with =f=
- Save settings with =s=
** Abstract specification label:abstractspec

Here I will discuss the theoretical segment. This is brief speculative look at
how we can and might further model the elements of interaction in a formal way.
It is split into three parts: data structures, physical elements and sensory
devices. This offers three different perspectives on the abstraction and
formalisation process.

*** Sensory devices
Identified here are four main parameters that one can think of as input or
sensor categories to the camera and processing algorithms. They are listed
below. These parameters can be variously tweaked and manipulated to interact
with a program. There can be cross over between these categories, such as with
pattern and shape, where patterns can be combinations of shapes and shapes which
make up patterns. They can also be combined in various ways so as to produce
interaction. In fact they will likely be most useful when combined as it
stretches the possible arrrangements and states that can be created.

#+ATTR_LATEX: :options [noitemsep]
- Colour
- Shape
- Location
- Relative position and arrangement
- Pattern

For example, as in the Colour Locator prototype different arrangements of
coloured shapes can act as marker points for location in the space. Different
combinations of these shapes can become symbolic for objects or images that the
program associates with them.

*** Data structures

Here are the theoretical data structures. These focus around the sensory
parameters described above.

#+begin_src cpp
Template Colour {
	vector<int> HSB_VALUE || RGB_VALUE;
	int ALPHA_VALUE;
}
Template Shape {
	int SIDES;
	vector<int> ANGLES;
}
Template Location {
	int X;
	int Y;
}
Template Pattern {
	vector<int> VALUES;
}
#+end_src
	
It can also be useful to think about what the data structures or higher level
combinations of the data structures might represent. What analogues of GUI
elements or other digital structures could they correspond to?

*** Physical elements

When building the Colour Locator system felt circles in five different
colours were used. This model could also be expanded beyond the scope of the
setup in the Colour Locator. Here we use a camera for detection but other kinds
of sensors would be equally useful. A depth sensor would be great for stability
only tracking colour that is at a specific distance from the sensor.

** Relative point mapping label:relpointalgo

Another element of the software outcome is this elementary algorithm for finding
pairs of points. It looks for pairs of points that are less than some distance
away from each other and then collects them and stores them in an array. This
algorithm is currently very slow, with a worst case algorithmic complexity of
roughly $O(k*n^2)$, where $n$ is the number of points (blobs) and $k$ is the
number of pairs [fn:4]. See Listing. ref:mapAlgo for the code.  cite:CormenThomas2009Ita 



#+caption: Algorithm for mapping and connecting points. label:mapAlgo
#+begin_src cpp
vector<vector<int>> Projector::findPairs(vector<cv::Point3f> &blobs) {
  vector<vector<int>> pairs;
  for (int i = 0; i < blobs.size(); i++) {
    for (int j = 0; j < blobs.size(); j++) {
      if (i != j) {
        float dist = ofDist(blobs[i].x, blobs[i].y, blobs[j].x, blobs[j].y);
        if (dist < 400) {
          // Loop over pairs
          bool _found = false;
          for (int k = 0; k < pairs.size(); k++) {
            vector<int>::iterator iti, itj;
            iti = find(pairs[k].begin(), pairs[k].end(), i);
            itj = find(pairs[k].begin(), pairs[k].end(), j);
            // Check pair has already been found
            if (iti != pairs[k].end() && itj != pairs[k].end()) {
              // Push pair to pairs
              // pairs.push_back({i, j});
              _found = true;
            }
          }
          if (!_found)
            pairs.push_back({i, j});
        }
      }
    }
  }
  return pairs;
}
#+end_src

** TODO Memory mapping prototype label:protomem
A prototypical experiment using the *CL* system has also been added. This is a
rudimentary interface for inputting text and assigning it to blobs in space.
When the =t= key is pressed two dialogue boxes are triggered. The first takes
the text you would like to assign and the second the id number of the blob you
would like to attach it to. The id number is sanitised to remove all but numeric
characters and checked so that there is blob with that id. See listing
ref:textinput to see the code. A ~std::map~, from the *C++ Standard Library*, is
used to associate a particular id number with a string of text. This acts as a
key value pair to retrieve the text for each associated id, stored in ~mapi~.
The maps are stored in the vector ~maps~. This rudimentary prototype is a
beginning for further developments of the system. See [[https://gitlab.doc.gold.ac.uk/ljame002/dynamic-cognition/-/commit/12beeff8b13bf4ea99d17e5a62a975efaa9794ab][Commit 12beeff8]].

#+caption: Text input using the OF ofSystemTextBoxDialog function. label:textinput
#+begin_src cpp
// testing text dialog
if(key=='t'){
    // Text dialog for input text
    string out = ofSystemTextBoxDialog("Enter some text:");
    // Text dialog for blob id number
    string idstring = ofSystemTextBoxDialog("Enter blob number:");
    // Sanitize by removing anything non alphanumeric from the idstring
    idstring = std::regex_replace(idstring, std::regex(R"([\D])"), "");
    // convert to int
    int blobid=-1;
    if(!idstring.empty()) blobid = stoi(idstring);
    // If blob exists store map and id
    if(blobid<ss->blobs.size() && blobid >= 0){
      map<int, string> tmpmap;
      tmpmap[blobid] = out;
      // cout << tmpmap[blobid] << endl;
      maps.push_back(tmpmap);
      mapi.push_back(blobid);
    } else {
        ofSystemAlertDialog("Blob does not exist");
        cout << "Blob does not exist\n";
    }
}
#+end_src
** API label:api

In the software outcome there is only a rudimentary "API" which is to access the
colour points. It can only be accessed inside the program itself at the current
time; there is no external API. There is no networking or connectivity. For each
detected blob you have its colour-id (a number from 1 to 5 corresponding to each
of the tracked colours), location (x and y coordinates). These active points
form the basis with which to build other augmentation on top of. In the current
version of the software these values are stored in a simple 3 dimensional vector
from the *openCv* library (~cv::Point3f~) (see Fig. ref:pseudoapi). \\

#+caption: Accessing the the parameters for point 'n' label:pseudoapi
#+begin_src cpp
ss->blobs[n].x // X position
ss->blobs[n].y // Y position
ss->blobs[n].z // Colour id
#+end_src

A simple proposed class for each blob seen in Fig. ref:pointClass. Having this
as a class would be useful for extensibility. It may remain a relatively simple
class as other processing could be done on top of the colour point detection.

#+caption: Proposed point class. label:pointClass
#+begin_src cpp
class colourPoint {
    public:
        colourPoint(loc, col_id){
             location=loc;
             colourId=col_id;
        }
    Point2f location;
    int colourId;
}
#+end_src

* TODO Creative process and software testing label:creativeproc

** Inspiration

The project has been heavily inspired by other software and research as
previously acknowledged. The basic idea behind this project is to describe and
implement an open-source version in /openFrameworks/ (OF). The projects that
inspired this one were physically unavailable; being in the US. /Paperprograms/
(PP) was available to download but as described below it was not suitable for
this idea. The objective was to aim for lower level architecture, in both language
and theory. Create a ground system with which to build many different types of
software on top of, all utilising the spatial model of interaction. \\

** Paperprograms testing

Paperprograms (PP) was a starting point for testing but it was stable enough to
develop on. It also suffers from being quite slow, due to the Computer Vision
and graphics being done in the browser (using a version of OpenCv compiled to
[[https://webassembly.org/][WebAssembly]]) cite:JpPaperPrograms. While WebAssembly has the scope for doing
high-performance computation in the browser but I found there was still a
significant lag from detecting papers to projecting back down on to them.
Another branch which had implemented bl ob detection on the GPU I also found to
be slow and unstable ([[https://github.com/janpaul123/paperprograms/pull/28][Link to pull request]]), this may have been due to my
lighting and camera setup. \\

After testing with /PP/ and finding it to be unstable and difficult to develop
on Cinder and OpenCV were considered. Another reason for moving away from /PP/
was it already being a fully fledged system in itself. It has potential for
developing some interesting tools collaboratively but for this solo project
working alone the social aspect would not be utilised. It is intended, like
Dynamicland as a tool for computing, but the goal of this project is to abstract
the model and open it to use beyond doing computing itself. Again DL and PP also
have this in spirit too but this aimed to be lower level.

** OpenCv and Cinder

Some early testing in vanilla c++ and the OpenCv library was also done. See [[https://gitlab.doc.gold.ac.uk/ljame002/dynamic-cognition/-/tree/master/opencv_testing][this
link]] for these files. This involved using OpenCv without another framework but
found OF had more available in close reach. Cinder (a similar c++ framework) was
also considered but certain libraries for graphics didn’t seem to be working so
stuff with OF.

** ofxPiMapper, projection mapping issue. label:projectionmapping

There is an open branch for called [[https://gitlab.doc.gold.ac.uk/ljame002/dynamic-cognition/-/tree/pimapper][pimapper]] which is where it is intended to
remerge some earlier commits. This early work was changing around the projector
setup to include [[https://ofxpimapper.com/][ofxPiMapper]] for doing some projection mapping. For the final
outcome no projection mapping is implemented as such, other that the controls
for the detection/projectoin area (See the GUI window in Fig. ref:guiwindow).
This only has controls for the controlling the size and position of the active
area, not the orientation or exact corners. Using the homography avaiable in
ofxPiMapper would mean for more control when changing this active region as well
as precise and simple mapping to it. In the current setup keystone calibration
on the projector is required which works fine but can be awkward to achieve (see
ref:videokeystone) for this.

** Design and development

Creative processes of this project has been goverened by building on the
principles originating in the background research and the specification. It was
also influenced as the project developed by technical compelxity in the given
timeframe and the outcome was refactored to include this.

*** Prototyping
The project is itself in prototypical form. More prototyping of actuation
reponses would have been useful ideation, as this has happened as the project
has developed rather than in a more structured manner. Before further
development further prototyping would be done, particularly of projection code.

** Other testing

*** Natural light versus synthetic.

As seen in Fig. ref:projncam. The camea and projector were setup next to a
hanging light. This was an important component for stability in tracking. At
night the light is obviously needed for lighting the space, but in the daytime
it is also necessary for creating a stability of light. If the natural light was
used only the colour tracking would be much less stable. If a body disrupted the
natural light source for a moment the tracking would struggle to pick up the
same colours after the disruption. With the hanging light turned on this was not
a problem. \\

A future design consideration relating to tracking and stability would be to
consider a sensor capable of tracking depth, such as a Kinect. This would allow
for detection objects at a certain range and would mean there would be less
disruption to the tracking. This was used in the MultiModal project
(ref:multimodalpro). In this project a higher resolution camera was chosen to do
the tracking without any depth sensing cabailities. This trade off could be
explored in further development. \\

*** Slow algorithms
As discussed in briefly in ref:relpointalgo there is issues with the complexity
of algorithm for finding pairs of points of a certain distance from eachother.
This could be improved fairly quickly with further development and insight. More
on algorithmic debugging and improvements are detailed in section ref:algoopt.

** Raspberry pi testing label:pitesting
Some testing on the raspberry pi has also been carrried out (see Fig.
ref:pitest). This was on a Raspberry pi 4 running Raspbian and openFrameworks
(armv6). There are points for accessibility here as it ran out of the box
without configuration. Speed was an issue though. There was a big delay in
frame-rate on the camera and the response on the projector window was lagging.
An interesting experiment this shows there is a good deal of effciency
improvements that could be made. It could be that it would always struggle on
the relatively slow processing capability of the pi but currently, it is
unusable on that platform.

# ** TODO Improvements to creative process

* TODO Debugging and problem solving
** Main technical issues label:technicaldebug
This chapter will deal with debugging and problem solving processes and how they
were used for various technical problems faced in this project.
*** Projection mapping
Originally implementing a form of projection mapping was proposed. A simplified
calibration tool is seen in the *Colour Locator*. This doesn't include any
advanced perspective point mapping but simply mapping the location and scale of
the located points. The perspective mapping is needed if the projector is not
exactly perpendicular to the ceiling. Currently *CL* relies on the projectors
built in keystone mapping. The calibration process is highlighted in the video
linked in Appendix III (ref:mappingvid).[fn:5] \\

The issue early on was that attempting to draw to the ~ofFbo~, OF's frame buffer
object would warp the contents of the frame buffer in the centre. This is
because no perspective mapping is implemented in the ~ofFbo~ object. See image
ref:mappingfail, linked in Appendix III, to see the issue. This was an
interesting and confusing bug, but the combination of internal location mapping
and the projector's mapping was the solution.

*** Algorithms and optimisation label:algoopt
The prototype is currently slower than it could be. Some optimisation in the
algorithms will be necessary for it to be more useful and extensible. As
discussed in section ref:relpointalgo, it's complexity means while it may run
for a few colour points it is not scalable when there are many points. "Depth
first search" looks like a good candidate for use in further development. \\

Optimisation could also be achieved by better utilisation of C++'s low level
memory capabilities. This has been used already, for example the State class is
instantiated as a ~shared_ptr~ which is passed to both the ~Projector~ and
~ofApp~ classes. This "smart pointer" means variables are not being moved around
by value, but each classes references the same shared instance.

*** Lighting issues label:lightissues
The ambient light in the space has been a source for a few bugs. As light
changes throughout the day this causes hues of the colours to change. If there
is change in light, target colours that are set are no longer calibrated to the
colours coming in through the camera. These issues motivated the controls on the
*GUI* window. These meant that parameters can be adjusted and re-calibrated
quickly and many different combinations could experiment with. The keyboard
shortcuts that controlled the exposure and reset the camera settings were
another useful part of this to quickly get the system working. An algorithm that
adjusts colours every so often could be useful to find the optimum values and
calibrate over time would be a useful solution. This is in the scope of future
development.
*** 
* Evaluation and conclusions label:evaluation
In conclusion a software prototype for spatial and tangible interaction is
presented. There is extensive scope for future development and various
strategies are detailed in the sections below.

** TODO Design iteration
The design of this project has iterated changed as it has progressed. Chapter
ref:creativeproc discusses this in detail. From an evaluative perspective there
the creative process has lacked in certain areas and achieved in others. Apart
from not quite achieving the more advanced system that was originally proposed,
it has stayed true to the original aims to design and formulate a spatial
computing environment. Where it has lacked is a more formal process along the
way. More thorough prototyping would have greatly benefited the project. This
would have sped up the greater development process by identifying pitfalls in
design that have taken time to develop.

** Practicality of current setup
*** Algorithms

Efficient algorithms for identifying clusters of points are an essential for
more advanced combinations of the sensory parameters. These will be needed for a
more stable system. As mentioned previously (ref:technicaldebug) currently
single blobs are can easily be disrupted by noise and interaction from the user.
Algorithmically locating higher level patterns amongst the coloured blobs will
solve this issue. In Paperprograms and Dynamicland sheets of paper are lined with
dots and the patterns of these colours encode an indentity for each program.
Similarly another algorithm could use some sort of pattern recognition to
identify such encodings. When tested on the Raspberry pi there it was unusably
slow (ref:pitesting) so there is plenty of room for improvement. \\

Envisaged in the Colour Locator different patterns of /colour points/ were to
represent corresponding “TUI (tangible user interface)”-like elements
cite:IshiiH2002Tbdt. For example a combination of three red points and two green
in a row instantiate a *handle class* which could be used to display a block of
text. Another pattern might represent an *image class* which displays an image.
This could lead to one development of the software, as a tool to create user
interfaces, on top of this spatial model.

*** Depth camera

A depth sensor could be useful as discussed (ref:lightissues) for a more stable
system. However, there would be a trade off between potential use and the
accessibility of the system, as this would add the need for more equipment and
cost. Some testing could be done with different and easily sourced sensors to
final a practical solution.

** Psychological aspects

The psychological aspects that were originally discussed and speculated on in
Chapter ref:background, have not been explored in detail. Apart from the overall
design being aimed at the psychological modes of interaction, this has been left
out of scope due to a focus on software engineering. Memory is one area that
this software could work well with. Due to our 3 dimensional lives, memory is
something that could benefit from interacting with a computer around a room as
opposed to a 2 dimensional monitor. While obviously not beneficial for all tasks
there is room for enquiry and application in this regard. \\

In the original aims there was some hope to build some simple tools with CL. One
idea was for a tool for improving memory, taking inspiration from Anki, a
program for spaced-repetition of flash cards. A couple of sources purport good
results for increasing memory capacity cite:BrownHanson,NielsenMich2018altm and
it seemed a good candidate translating into CL’s model.
** Secondary outcome

The theoretical outcome (ref:abstractspec) was a brief addition to the software
Colour Locator (CL). Intended as a formal conceptual summary of the design and
developmental aims of CL it aims for to better describe the model, for the
discussion of it's potential use cases. It also offers a description of the
problems this project aims to address; attempting to add a more robust
specification for development of the prototype.

** TODO Social aspects. Proposed social evaluation

Originally social surveying was to be used in evaluation. This would involve
surveys and conversation analysis cite:SharpHelen2019IDBH to assess various
aspects of the system. This would regard usability, any benefits and
difficulties with use. This was not carried out as CL will only be of interest
to developers in its current state. No user testing in person has been tried out
due to not being able to get appropriate users to physically use the work and
also it's incomplete nature. A survey of proposed survey questions are listed
below. These aim to gauge the potential users interest in the use of such
software and thereby areas for system improvement.

*** Proposed survey questions for potential developers
#+ATTR_LATEX: :options [noitemsep]
- Could you envisage using integrating this interaction model into your
  workflow?
- Are you interested in open-source software development?
*** Proposed survey questions for general users
#+ATTR_LATEX: :options [noitemsep]
- Could you envisage using integrating this interaction model into your current
  usage of computers?
- Do you feel that this would help you have a healthier relationships when using
  your computer? {Rate on a scale from 1(not at all useful) to 5(very useful)}
- Do you feel like you could have a healthier or more productive relationship
  with your computer (Desktop or laptop).
** TODO Durability
The implementation has been fairly well tested, identifying different points for
improvement and progression of the software. As the system progresses beyond the
scope of this report, more bugs will probably be found. Overall the prototypical
nature of the software means that improvement is inevitable. The aim for
creating a system to improve on the stability and speed of Paperprograms was not
achieved. This is one the major failings of the project in that it is not
offering, in it's current state anything directly usable to this area of
interaction-design.

** TODO Future scope for software development
Here are some finalised discussion points for progression of the project.

** Reflection on process
*** Simplicity of prototype and large room for development
*** Needs improvement to become significant
* Research notes :noexport:
** SAGE GUIDEBOOK for digital technology research
*** Theories of embodiment in HCI
*** Haptic interfaces
"the widgets cannot provide the haptic response that physical objects do when
touched or clicked. By adding haptic feedback to user interfaces, we can
recreate the physical sensation of pressing a button, holding a ball or even
create completely new touch sensations."

*** ethno methodology
- Propose and trial ethnomethodological framework for project evaluation
* Links :noexport:
- http://web.mit.edu/ebj/www/JPER.pdf - similar project - urban planning workbench
* Appendices
** Appendix I: Additional images label:additionalimages
#+caption: Camera and projector secured on ceiling. label:projncam
#+ATTR_LATEX: :width 15cm :float
[[file:assets/camproj.jpg]]

#+caption: Detection and Corresponding projection. label:serve_project
#+ATTR_LATEX: :width 15cm :float t
[[file:assets/serve_project.jpg]]

#+caption: Testing on Raspberry Pi 4. label:pitest
#+ATTR_LATEX: :width 17cm :float 
[[file:assets/pitest.jpg]]

#+caption: Projection mapping bug. label:mappingfail
#+ATTR_LATEX: :width 17cm :float 
[[file:assets/nomapping.jpg]]
** Appendix II: Code repository links label:codelinks
*** Code repository on [[https://gitlab.doc.gold.ac.uk/ljame002/dynamic-cognition][Gitlab]]. 
*** Code repository on [[https://github.com/locua/spatial-memoriser][Github.]]

** Appendix III: Links to video documentation label:videodoc
*** label:framebufbug
*** label:mappingvid
* Bibliography :ignore:
bibliographystyle:ieeetr
bibliography:references.bib
* Footnotes

[fn:5]  [[https://ofxpimapper.com/][ofxPiMapper]] was one solution with projective transformation algorithms.
Development with this as previously mentioned could be useful as it removes the
need for keystone calibration which is limited on some projectors.

[fn:4] This may not be precise but the main takeaway is that is not scalable. It
runs well with a few points and tight thresholds but it becomes very slow if there
is many points of interest.

[fn:3] This is the third class which allows for the sharing of variables and
objects between the ~GUI~ and ~Projector~ classes. It is consists of a Shared
Pointer to the State class, ~shared_ptr<State>~, which is passed as an argument
to the ~GUI~ and ~Projector~ classes.


[fn:1] Due in part to the ongoing Coronavirus pandemic.
[fn:2] This document is produced with org-mode.



